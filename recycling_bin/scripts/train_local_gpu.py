"""
Script de Entrenamiento Local para GPU NVIDIA GTX 1060 (6GB VRAM)

Este script estÃ¡ optimizado para entrenar mT5-small en una GPU con memoria limitada.

Optimizaciones aplicadas:
- Batch size reducido (4 en lugar de 8)
- Gradient accumulation (simula batch mÃ¡s grande)
- PrecisiÃ³n mixta (fp16) para ahorrar memoria
- Gradient checkpointing (reduce uso de memoria)
- EvaluaciÃ³n menos frecuente

Requisitos:
- NVIDIA GTX 1060 (6GB VRAM)
- CUDA instalado
- PyTorch con soporte CUDA
"""

import os
import json
import torch
from pathlib import Path
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    TrainingArguments,
    Trainer
)
from datasets import Dataset, DatasetDict
import evaluate
import numpy as np

# Verificar GPU
print("=" * 60)
print("VERIFICACIÃ“N DE HARDWARE")
print("=" * 60)

if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"âœ… GPU detectada: {gpu_name}")
    print(f"ğŸ’¾ Memoria GPU: {gpu_memory:.1f} GB")
    
    # Limpiar cachÃ© de GPU
    torch.cuda.empty_cache()
    print("ğŸ§¹ CachÃ© de GPU limpiada")
else:
    print("âŒ GPU no detectada")
    print("âš ï¸ El entrenamiento serÃ¡ MUY lento en CPU")
    response = input("Â¿Continuar de todos modos? (y/n): ")
    if response.lower() != 'y':
        exit()

print()

# ConfiguraciÃ³n
DATA_DIR = Path("data/training_corpus/phase1")
OUTPUT_DIR = Path("models/checkpoints_local")
FINAL_MODEL_DIR = Path("models/latin_translator_v1.0_local")

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
FINAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)

MODEL_NAME = "google/mt5-small"

# ============================================
# CARGAR DATOS
# ============================================

print("=" * 60)
print("CARGANDO DATOS")
print("=" * 60)

def load_data():
    """Carga datos de entrenamiento."""
    train_path = DATA_DIR / "train.json"
    val_path = DATA_DIR / "validation.json"
    
    if not train_path.exists() or not val_path.exists():
        print(f"âŒ Error: No se encontraron los archivos de datos")
        print(f"   Esperado en: {DATA_DIR}")
        print(f"   Ejecuta primero: python scripts/download_training_corpus.py")
        exit()
    
    with open(train_path, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    with open(val_path, 'r', encoding='utf-8') as f:
        val_data = json.load(f)
    
    train_dataset = Dataset.from_dict({
        'latin': [item['latin'] for item in train_data],
        'spanish': [item['spanish'] for item in train_data]
    })
    
    val_dataset = Dataset.from_dict({
        'latin': [item['latin'] for item in val_data],
        'spanish': [item['spanish'] for item in val_data]
    })
    
    return DatasetDict({
        'train': train_dataset,
        'validation': val_dataset
    })

dataset = load_data()

print(f"âœ… Datos cargados:")
print(f"   - Entrenamiento: {len(dataset['train'])} pares")
print(f"   - ValidaciÃ³n: {len(dataset['validation'])} pares")
print()

# ============================================
# CARGAR MODELO
# ============================================

print("=" * 60)
print("CARGANDO MODELO")
print("=" * 60)

print(f"ğŸ“¥ Descargando {MODEL_NAME}...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

# Habilitar gradient checkpointing para ahorrar memoria
model.gradient_checkpointing_enable()
model.config.use_cache = False  # Incompatible con gradient checkpointing

print(f"âœ… Modelo cargado")
print(f"ğŸ“Š ParÃ¡metros: {model.num_parameters():,}")
print()

# ============================================
# PREPROCESAMIENTO
# ============================================

print("=" * 60)
print("PREPROCESANDO DATOS")
print("=" * 60)

def preprocess_function(examples):
    """Preprocesa los datos para mT5."""
    inputs = ["translate Latin to Spanish: " + text for text in examples['latin']]
    targets = examples['spanish']
    
    model_inputs = tokenizer(
        inputs,
        max_length=48,
        truncation=True,
        padding='max_length'
    )
    
    labels = tokenizer(
        targets,
        max_length=48,
        truncation=True,
        padding='max_length'
    )
    
    model_inputs['labels'] = labels['input_ids']
    
    return model_inputs

tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset['train'].column_names
)

print("âœ… Datos preprocesados")
print()

# ============================================
# CONFIGURACIÃ“N DE ENTRENAMIENTO
# ============================================

print("=" * 60)
print("CONFIGURACIÃ“N DE ENTRENAMIENTO")
print("=" * 60)

# ConfiguraciÃ³n optimizada para GTX 1060 (6GB)
training_args = TrainingArguments(
    # Directorios
    output_dir=str(OUTPUT_DIR),
    logging_dir=str(OUTPUT_DIR / "logs"),
    
    # Guardado de checkpoints
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,  # Solo mantener Ãºltimos 2 checkpoints
    
    # EvaluaciÃ³n
    eval_strategy="steps",
    eval_steps=500,
    
    # HiperparÃ¡metros optimizados para 6GB VRAM (ajustado por OOM)
    learning_rate=5e-5,
    per_device_train_batch_size=1,      # Reducido de 2 a 1
    per_device_eval_batch_size=1,       # Reducido de 2 a 1
    gradient_accumulation_steps=16,     # Aumentado a 16 (simula batch_size=16)
    num_train_epochs=20,
    
    # Optimizaciones de memoria
    fp16=True,                           # PrecisiÃ³n mixta
    gradient_checkpointing=True,         # Ahorra memoria
    optim="adafactor",                   # Optimizador que usa menos memoria
    eval_accumulation_steps=1,           # Mover a CPU frecuentemente para evitar OOM
    
    # Logging
    logging_steps=50,
    
    # Otros
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    report_to="none",
    
    # Desactivar features que usan memoria extra
    dataloader_pin_memory=False,
    dataloader_num_workers=0,
)

print("âœ… ConfiguraciÃ³n lista")
print(f"\nğŸ“Š ParÃ¡metros:")
print(f"   - Batch size efectivo: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"   - Ã‰pocas: {training_args.num_train_epochs}")
print(f"   - Learning rate: {training_args.learning_rate}")
print(f"   - FP16: {training_args.fp16}")
print(f"   - Gradient checkpointing: {training_args.gradient_checkpointing}")
print()

# ============================================
# MÃ‰TRICAS
# ============================================

metric = evaluate.load("sacrebleu")

def compute_metrics(eval_preds):
    """Calcula BLEU score."""
    preds, labels = eval_preds
    
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    result = metric.compute(
        predictions=decoded_preds,
        references=[[label] for label in decoded_labels]
    )
    
    return {"bleu": result["score"]}

# ============================================
# ENTRENAMIENTO
# ============================================

print("=" * 60)
print("INICIANDO ENTRENAMIENTO")
print("=" * 60)

# Verificar checkpoints existentes
checkpoints = [d for d in OUTPUT_DIR.iterdir() if d.is_dir() and d.name.startswith("checkpoint-")]

if checkpoints:
    latest = sorted(checkpoints, key=lambda x: int(x.name.split("-")[1]))[-1]
    print(f"ğŸ”„ Reanudando desde: {latest}")
    resume_from = str(latest)
else:
    print("ğŸ†• Iniciando desde cero")
    resume_from = None

# Crear Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['validation'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print()
print("ğŸš€ Entrenamiento iniciado...")
print(f"â±ï¸ Tiempo estimado: ~2-3 horas (para 1,000 pares)")
print(f"ğŸ’¾ Checkpoints en: {OUTPUT_DIR}")
print(f"ğŸ“Š Puedes monitorear en: {OUTPUT_DIR / 'logs'}")
print()
print("=" * 60)
print()

# ENTRENAR
try:
    trainer.train(resume_from_checkpoint=resume_from)
    
    print()
    print("=" * 60)
    print("âœ… ENTRENAMIENTO COMPLETADO")
    print("=" * 60)
    
except KeyboardInterrupt:
    print()
    print("=" * 60)
    print("âš ï¸ ENTRENAMIENTO INTERRUMPIDO")
    print("=" * 60)
    print("ğŸ’¾ El progreso se guardÃ³ en el Ãºltimo checkpoint")
    print("ğŸ”„ Puedes reanudar ejecutando este script nuevamente")
    exit()

except RuntimeError as e:
    if "out of memory" in str(e).lower():
        print()
        print("=" * 60)
        print("âŒ ERROR: MEMORIA GPU INSUFICIENTE")
        print("=" * 60)
        print("ğŸ’¡ Soluciones:")
        print("   1. Reduce per_device_train_batch_size a 2")
        print("   2. Aumenta gradient_accumulation_steps a 8")
        print("   3. Reduce max_length de 128 a 64")
        print("   4. Cierra otros programas que usen la GPU")
        exit()
    else:
        raise

# ============================================
# GUARDAR MODELO FINAL
# ============================================

print()
print("=" * 60)
print("GUARDANDO MODELO FINAL")
print("=" * 60)

trainer.save_model(str(FINAL_MODEL_DIR))
tokenizer.save_pretrained(str(FINAL_MODEL_DIR))

print(f"âœ… Modelo guardado en: {FINAL_MODEL_DIR}")
print()

# ============================================
# EVALUACIÃ“N FINAL
# ============================================

print("=" * 60)
print("EVALUACIÃ“N FINAL")
print("=" * 60)

eval_results = trainer.evaluate()

print(f"ğŸ“Š Resultados:")
print(f"   - Loss: {eval_results['eval_loss']:.4f}")
print(f"   - BLEU: {eval_results['eval_bleu']:.2f}")
print()

if eval_results['eval_bleu'] > 30:
    print("ğŸ‰ Â¡Excelente calidad!")
elif eval_results['eval_bleu'] > 20:
    print("âœ… Buena calidad")
elif eval_results['eval_bleu'] > 10:
    print("âš ï¸ Calidad aceptable - considera mÃ¡s datos")
else:
    print("âŒ Calidad baja - necesitas corpus mÃ¡s grande")

print()
print("=" * 60)
print("ğŸ‰ Â¡ENTRENAMIENTO FINALIZADO!")
print("=" * 60)
print()
print(f"ğŸ“ Modelo final: {FINAL_MODEL_DIR}")
print()
print("ğŸš€ Para usar el modelo:")
print(f"   from transformers import AutoModelForSeq2SeqLM, AutoTokenizer")
print(f"   model = AutoModelForSeq2SeqLM.from_pretrained('{FINAL_MODEL_DIR}')")
print(f"   tokenizer = AutoTokenizer.from_pretrained('{FINAL_MODEL_DIR}')")
print()
