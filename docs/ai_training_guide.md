# GuÃ­a Completa: Entrenamiento de IA para TraducciÃ³n LatÃ­nâ†’EspaÃ±ol

## Ãndice
1. [IntroducciÃ³n](#introducciÃ³n)
2. [PreparaciÃ³n de Datos](#preparaciÃ³n-de-datos)
3. [ConfiguraciÃ³n de Google Colab](#configuraciÃ³n-de-google-colab)
4. [Entrenamiento por Sesiones](#entrenamiento-por-sesiones)
5. [IntegraciÃ³n con la AplicaciÃ³n](#integraciÃ³n-con-la-aplicaciÃ³n)

---

## IntroducciÃ³n

### Objetivo
Entrenar un modelo de IA especializado que:
- Traduzca latÃ­n clÃ¡sico a espaÃ±ol
- Sea relativamente compacto (~500MB)
- Tenga alta calidad en textos clÃ¡sicos (Caesar, Cicero, Virgilio, etc.)

### Arquitectura Elegida
**mT5-small** (Google's multilingual T5)
- **TamaÃ±o**: ~300MB
- **Ventajas**: 
  - Pre-entrenado en 101 idiomas (incluye conocimiento de estructuras latinas)
  - Arquitectura encoder-decoder ideal para traducciÃ³n
  - FÃ¡cil de afinar (fine-tune)
- **Desventajas**: 
  - Necesita corpus de entrenamiento de calidad
  - Requiere ~8-12 horas de entrenamiento en GPU

### Sistema de Checkpoints
El entrenamiento se guardarÃ¡ cada 500 pasos en Google Drive, permitiendo:
- Pausar y reanudar en cualquier momento
- No perder progreso si se desconecta Colab
- Evaluar modelos intermedios

---

## PreparaciÃ³n de Datos

### Fase 1: RecopilaciÃ³n de Corpus

#### Fuentes Recomendadas

**1. Vulgata (Biblia Latina)**
- **Ventaja**: Texto completo con mÃºltiples traducciones espaÃ±olas
- **TamaÃ±o**: ~800,000 palabras
- **Descarga**: [Sacred Texts](https://sacred-texts.com/bib/vul/)

**2. Perseus Digital Library**
- **Textos**: Caesar, Cicero, Virgilio, Ovidio
- **Formato**: XML con traducciones
- **URL**: https://www.perseus.tufts.edu/hopper/

**3. Tus Datos Existentes**
- `data/texts/classical_samples_translated.json` (18 pares)
- Textos de Maud Reed (cuando los traduzcas)

#### Estructura de Datos Objetivo

```json
[
  {
    "latin": "Gallia est omnis divisa in partes tres.",
    "spanish": "Toda la Galia estÃ¡ dividida en tres partes.",
    "source": "caesar_bg_1_1",
    "difficulty": 3
  },
  ...
]
```

### Fase 2: Script de PreparaciÃ³n

Crearemos un script local para:
1. Descargar corpus
2. Limpiar y normalizar textos
3. Crear splits de entrenamiento/validaciÃ³n (90/10)
4. Exportar en formato compatible con Hugging Face

**TamaÃ±o objetivo**: 20,000-50,000 pares latÃ­n-espaÃ±ol

---

## ConfiguraciÃ³n de Google Colab

### Paso 1: Crear Notebook

1. Ve a [Google Colab](https://colab.research.google.com/)
2. Crea un nuevo notebook: `Latin_Spanish_Translator_Training.ipynb`
3. Conecta a Google Drive para persistencia

### Paso 2: ConfiguraciÃ³n Inicial

```python
# ============================================
# SECCIÃ“N 1: CONFIGURACIÃ“N Y CONEXIÃ“N
# ============================================
# Â¿Por quÃ©? Necesitamos acceso a Google Drive para guardar checkpoints

from google.colab import drive
drive.mount('/content/drive')

# Crear directorio de trabajo
!mkdir -p /content/drive/MyDrive/latin_translator
%cd /content/drive/MyDrive/latin_translator

print("âœ… Google Drive conectado")
print("ðŸ“ Directorio de trabajo: /content/drive/MyDrive/latin_translator")
```

### Paso 3: InstalaciÃ³n de Dependencias

```python
# ============================================
# SECCIÃ“N 2: INSTALACIÃ“N DE LIBRERÃAS
# ============================================
# Â¿Por quÃ© cada una?
# - transformers: Framework de Hugging Face para modelos de lenguaje
# - datasets: Manejo eficiente de datos de entrenamiento
# - sentencepiece: TokenizaciÃ³n requerida por mT5
# - sacrebleu: MÃ©trica de evaluaciÃ³n de traducciÃ³n (BLEU score)

!pip install -q transformers datasets sentencepiece sacrebleu

print("âœ… Dependencias instaladas")
```

### Paso 4: Verificar GPU

```python
# ============================================
# SECCIÃ“N 3: VERIFICACIÃ“N DE HARDWARE
# ============================================
# Â¿Por quÃ©? Asegurarnos de tener GPU disponible

import torch

if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"âœ… GPU disponible: {gpu_name}")
    print(f"ðŸ’¾ Memoria GPU: {gpu_memory:.1f} GB")
else:
    print("âŒ GPU no disponible. Ve a Runtime > Change runtime type > GPU")
```

---

## Entrenamiento por Sesiones

### Arquitectura del Sistema de Checkpoints

```
/content/drive/MyDrive/latin_translator/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.json          # Datos de entrenamiento
â”‚   â”œâ”€â”€ validation.json     # Datos de validaciÃ³n
â”‚   â””â”€â”€ test.json           # Datos de prueba
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ checkpoint-500/     # Guardado cada 500 pasos
â”‚   â”œâ”€â”€ checkpoint-1000/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ final_model/            # Modelo final entrenado
â””â”€â”€ training_log.txt        # Registro de progreso
```

### Paso 5: Cargar o Preparar Datos

```python
# ============================================
# SECCIÃ“N 4: CARGA DE DATOS
# ============================================
# Â¿Por quÃ© este formato?
# - JSON es fÃ¡cil de editar y verificar manualmente
# - Hugging Face Datasets puede cargarlo directamente
# - Permite aÃ±adir metadatos (source, difficulty)

import json
from datasets import Dataset, DatasetDict

# OpciÃ³n A: Cargar datos existentes
def load_training_data():
    """
    Carga los datos de entrenamiento desde archivos JSON.
    
    Estructura esperada:
    [
      {"latin": "...", "spanish": "..."},
      ...
    ]
    """
    with open('data/train.json', 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    with open('data/validation.json', 'r', encoding='utf-8') as f:
        val_data = json.load(f)
    
    # Convertir a formato Hugging Face Dataset
    train_dataset = Dataset.from_dict({
        'latin': [item['latin'] for item in train_data],
        'spanish': [item['spanish'] for item in train_data]
    })
    
    val_dataset = Dataset.from_dict({
        'latin': [item['latin'] for item in val_data],
        'spanish': [item['spanish'] for item in val_data]
    })
    
    dataset = DatasetDict({
        'train': train_dataset,
        'validation': val_dataset
    })
    
    return dataset

# Cargar datos
dataset = load_training_data()

print(f"âœ… Datos cargados:")
print(f"   - Entrenamiento: {len(dataset['train'])} pares")
print(f"   - ValidaciÃ³n: {len(dataset['validation'])} pares")
print(f"\nðŸ“ Ejemplo:")
print(f"   Latin: {dataset['train'][0]['latin']}")
print(f"   Spanish: {dataset['train'][0]['spanish']}")
```

### Paso 6: Preparar Modelo y Tokenizer

```python
# ============================================
# SECCIÃ“N 5: INICIALIZACIÃ“N DEL MODELO
# ============================================
# Â¿Por quÃ© mT5-small?
# - TamaÃ±o manejable (~300MB)
# - Pre-entrenado en mÃºltiples idiomas
# - Arquitectura probada para traducciÃ³n

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

MODEL_NAME = "google/mt5-small"

# Cargar tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Cargar modelo
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

print(f"âœ… Modelo cargado: {MODEL_NAME}")
print(f"ðŸ“Š ParÃ¡metros: {model.num_parameters():,}")
```

### Paso 7: Preprocesamiento de Datos

```python
# ============================================
# SECCIÃ“N 6: PREPROCESAMIENTO
# ============================================
# Â¿Por quÃ© este preprocesamiento?
# - mT5 necesita prefijos de tarea ("translate Latin to Spanish: ")
# - TokenizaciÃ³n convierte texto a IDs numÃ©ricos
# - Padding asegura que todos los ejemplos tengan la misma longitud

def preprocess_function(examples):
    """
    Preprocesa los datos para el modelo mT5.
    
    Args:
        examples: Batch de ejemplos con 'latin' y 'spanish'
    
    Returns:
        Dict con input_ids, attention_mask, labels
    """
    # AÃ±adir prefijo de tarea
    inputs = ["translate Latin to Spanish: " + text for text in examples['latin']]
    targets = examples['spanish']
    
    # Tokenizar inputs
    model_inputs = tokenizer(
        inputs,
        max_length=128,      # Longitud mÃ¡xima de entrada
        truncation=True,     # Truncar si es muy largo
        padding='max_length' # Rellenar si es muy corto
    )
    
    # Tokenizar targets
    labels = tokenizer(
        targets,
        max_length=128,
        truncation=True,
        padding='max_length'
    )
    
    model_inputs['labels'] = labels['input_ids']
    
    return model_inputs

# Aplicar preprocesamiento
tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset['train'].column_names
)

print("âœ… Datos preprocesados")
```

### Paso 8: Configurar Entrenamiento con Checkpoints

```python
# ============================================
# SECCIÃ“N 7: CONFIGURACIÃ“N DE ENTRENAMIENTO
# ============================================
# Â¿Por quÃ© estos parÃ¡metros?
# - output_dir: DÃ³nde guardar checkpoints (en Google Drive)
# - save_steps: Guardar cada 500 pasos (cada ~30 min)
# - evaluation_strategy: Evaluar cada 500 pasos
# - learning_rate: Tasa de aprendizaje conservadora
# - num_train_epochs: 3 Ã©pocas completas (~8-12 horas)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    # Directorio de salida (en Google Drive para persistencia)
    output_dir="./checkpoints",
    
    # Estrategia de guardado
    save_strategy="steps",
    save_steps=500,                    # Guardar cada 500 pasos
    save_total_limit=5,                # Mantener solo Ãºltimos 5 checkpoints
    
    # Estrategia de evaluaciÃ³n
    evaluation_strategy="steps",
    eval_steps=500,                    # Evaluar cada 500 pasos
    
    # HiperparÃ¡metros
    learning_rate=5e-5,                # Tasa de aprendizaje
    per_device_train_batch_size=8,    # TamaÃ±o de batch (ajustar segÃºn GPU)
    per_device_eval_batch_size=8,
    num_train_epochs=3,                # NÃºmero de Ã©pocas
    
    # Optimizaciones
    fp16=True,                         # PrecisiÃ³n mixta (mÃ¡s rÃ¡pido)
    gradient_accumulation_steps=2,     # Acumular gradientes
    
    # Logging
    logging_dir="./logs",
    logging_steps=100,
    
    # Otros
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    report_to="none"                   # No usar wandb/tensorboard
)

print("âœ… ConfiguraciÃ³n de entrenamiento lista")
print(f"ðŸ“Š Pasos totales estimados: {len(tokenized_dataset['train']) // 8 * 3}")
print(f"ðŸ’¾ Checkpoints se guardarÃ¡n en: ./checkpoints")
```

### Paso 9: FunciÃ³n de MÃ©tricas

```python
# ============================================
# SECCIÃ“N 8: MÃ‰TRICAS DE EVALUACIÃ“N
# ============================================
# Â¿Por quÃ© BLEU?
# - MÃ©trica estÃ¡ndar para traducciÃ³n automÃ¡tica
# - Compara traducciÃ³n generada vs. referencia
# - Rango 0-100 (mÃ¡s alto = mejor)

import numpy as np
from datasets import load_metric

metric = load_metric("sacrebleu")

def compute_metrics(eval_preds):
    """
    Calcula mÃ©tricas de evaluaciÃ³n (BLEU score).
    
    Args:
        eval_preds: Tupla de (predictions, labels)
    
    Returns:
        Dict con mÃ©tricas
    """
    preds, labels = eval_preds
    
    # Decodificar predicciones
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    
    # Decodificar labels (reemplazar -100 con pad_token_id)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # Calcular BLEU
    result = metric.compute(
        predictions=decoded_preds,
        references=[[label] for label in decoded_labels]
    )
    
    return {"bleu": result["score"]}

print("âœ… FunciÃ³n de mÃ©tricas configurada")
```

### Paso 10: Iniciar/Reanudar Entrenamiento

```python
# ============================================
# SECCIÃ“N 9: ENTRENAMIENTO
# ============================================
# Â¿CÃ³mo funciona la reanudaciÃ³n?
# - Si existe un checkpoint, Trainer lo carga automÃ¡ticamente
# - El entrenamiento continÃºa desde el Ãºltimo paso guardado
# - No se pierde progreso entre sesiones

import os

# Verificar si hay checkpoints existentes
checkpoints = [d for d in os.listdir("./checkpoints") if d.startswith("checkpoint-")]

if checkpoints:
    # Ordenar por nÃºmero de paso
    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split("-")[1]))[-1]
    checkpoint_path = f"./checkpoints/{latest_checkpoint}"
    print(f"ðŸ”„ Reanudando desde: {checkpoint_path}")
    resume_from_checkpoint = checkpoint_path
else:
    print("ðŸ†• Iniciando entrenamiento desde cero")
    resume_from_checkpoint = None

# Crear Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['validation'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Iniciar entrenamiento
print("ðŸš€ Iniciando entrenamiento...")
print("â±ï¸ Tiempo estimado: 8-12 horas")
print("ðŸ’¡ Puedes cerrar esta pestaÃ±a. El progreso se guarda en Google Drive.")

trainer.train(resume_from_checkpoint=resume_from_checkpoint)

print("âœ… Entrenamiento completado!")
```

### Paso 11: Guardar Modelo Final

```python
# ============================================
# SECCIÃ“N 10: GUARDAR MODELO FINAL
# ============================================

# Guardar modelo final
trainer.save_model("./final_model")
tokenizer.save_pretrained("./final_model")

print("âœ… Modelo final guardado en: ./final_model")
print("ðŸ“¦ TamaÃ±o aproximado: ~300MB")
print("\nðŸ“¥ Para usar en tu aplicaciÃ³n:")
print("   1. Descarga la carpeta 'final_model' de Google Drive")
print("   2. ColÃ³cala en: /home/diego/Projects/latin-python/models/")
print("   3. Carga con: AutoModelForSeq2SeqLM.from_pretrained('models/final_model')")
```

---

## IntegraciÃ³n con la AplicaciÃ³n

### Paso 12: Script de IntegraciÃ³n Local

Crearemos un script en tu proyecto para usar el modelo entrenado:

```python
# utils/latin_translator.py

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

class LatinTranslator:
    """
    Traductor latÃ­nâ†’espaÃ±ol usando modelo entrenado.
    """
    
    def __init__(self, model_path="models/final_model"):
        """
        Inicializa el traductor.
        
        Args:
            model_path: Ruta al modelo entrenado
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)
        
    def translate(self, latin_text: str) -> str:
        """
        Traduce texto latino a espaÃ±ol.
        
        Args:
            latin_text: Texto en latÃ­n
            
        Returns:
            TraducciÃ³n en espaÃ±ol
        """
        # Preparar input
        input_text = f"translate Latin to Spanish: {latin_text}"
        inputs = self.tokenizer(
            input_text,
            return_tensors="pt",
            max_length=128,
            truncation=True
        ).to(self.device)
        
        # Generar traducciÃ³n
        outputs = self.model.generate(
            **inputs,
            max_length=128,
            num_beams=4,           # Beam search para mejor calidad
            early_stopping=True
        )
        
        # Decodificar
        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return translation
```

### Uso en la AplicaciÃ³n

```python
# En analyze_and_import_maud_reed.py o similar

from utils.latin_translator import LatinTranslator

# Inicializar traductor
translator = LatinTranslator()

# Usar en anÃ¡lisis
for sentence in sentences:
    # AnÃ¡lisis sintÃ¡ctico (LatinCy)
    analysis = analyzer.analyze_sentence(sentence)
    
    # TraducciÃ³n (modelo entrenado)
    translation = translator.translate(sentence)
    analysis.spanish_translation = translation
    
    # Guardar
    session.add(analysis)
```

---

## Monitoreo del Progreso

### Durante el Entrenamiento

El entrenamiento imprimirÃ¡ logs cada 100 pasos:

```
Step 100: loss=2.456, eval_loss=2.123, bleu=12.3
Step 200: loss=2.234, eval_loss=2.001, bleu=15.7
...
```

**InterpretaciÃ³n**:
- `loss`: Error en datos de entrenamiento (debe bajar)
- `eval_loss`: Error en datos de validaciÃ³n (debe bajar)
- `bleu`: Calidad de traducciÃ³n (debe subir, objetivo: >30)

### Entre Sesiones

Para verificar progreso sin entrenar:

```python
# Ver Ãºltimo checkpoint
!ls -lh checkpoints/

# Cargar y probar modelo intermedio
from transformers import pipeline

translator = pipeline(
    "translation",
    model="./checkpoints/checkpoint-1000",
    device=0
)

test_sentence = "Gallia est omnis divisa in partes tres."
result = translator(f"translate Latin to Spanish: {test_sentence}")
print(result[0]['translation_text'])
```

---

## PrÃ³ximos Pasos

1. **Preparar datos**: Crear `train.json` y `validation.json`
2. **Ejecutar notebook**: Seguir secciones 1-11
3. **Monitorear**: Revisar cada 2-3 horas
4. **Descargar modelo**: Cuando termine, descargar de Google Drive
5. **Integrar**: Usar `LatinTranslator` en tu aplicaciÃ³n

Â¿Listo para empezar? Puedo ayudarte con:
- Script para preparar datos desde tus fuentes
- Notebook de Colab completo y listo para ejecutar
- Debugging durante el entrenamiento
